{"name":"Zen-of-test-suites","tagline":"Testing Applications Instead of Libraries","body":"# The Zen of Application Test Suites\r\n\r\nSerious testing for serious software (work in progress)\r\n\r\n## Note\r\n\r\nThis document is about testing applications — it's not about how to write\r\ntests. Application test suites require a different, more disciplined approach\r\nthan library test suites. I describe common misfeatures experienced in large\r\napplication test suites and follow with recommendations on best practices.\r\nMuch of what I describe below is generic and applies to test suites written in\r\nany programming language, despite many examples being written in Perl.\r\n\r\n# Introduction\r\n\r\nI often speak with developers who take a new job and they describe a Web site\r\nbuilt out of a bunch of separate scripts scattered randomly through\r\ndirectories, lots of duplicated code, poor use of modules, with embedded SQL\r\nand printing HTTP headers and HTML directly. The developers shake their head\r\nin despair, but grudgingly admit an upside: job security. New features are\r\ntime-consuming to add, changes are difficult to implement and may have\r\nwide-ranging side-effects, and reorganizing the codebase to have a proper\r\nseparation of concerns, to make it cheaper and safer to hack on, will take\r\nlots and lots of time.\r\n\r\nA bunch of randomly scattered scripts, no separation of concerns, lots of\r\nduplicated code, poor use of modules, SQL embedded directly in them? Does this\r\nsound familiar? It's your standard test suite. We're horrified by this in the\r\ncode, but don't bat an eyelash at the test suite.\r\n\r\nPart of this is because much, if not most, of the testing examples we find\r\nfocus on testing distributions, not applications. If you were to look at the\r\ntests for my module\r\n[DBIx::Class::EasyFixture](https://github.com/Ovid/dbix-class-easyfixture),\r\nyou'd see the following tests:\r\n\r\n    00-load.t\r\n    basic.t\r\n    definitions.t\r\n    groups.t\r\n    load_multiple_fixtures.t\r\n    many_to_many.t\r\n    no_transactions.t\r\n    one_to_many.t\r\n    one_to_one.t\r\n\r\nThese tests were added one by one, as I added new features to\r\n`DBIx::Class::EasyFixture` and each `*.t` file represents (more or less) a\r\ndifferent feature.\r\n\r\nFor a small distribution, this isn't too bad because it's very easy to keep it\r\nall in your head. With only nine files, it's trivial to glance at them, or\r\ngrep them, to figure out where the relevant tests are. Applications, however,\r\nare a different story. This is the number of files from one of my customer's\r\ntest suites:\r\n\r\n    $ find t -type f | wc -l\r\n    288\r\n\r\nThat's actually fairly small. One codebase I worked on had close to a million\r\nlines of code with thousands of test scripts. You couldn't hold the codebase\r\nin your head, you're couldn't *glance* at the the tests to figure out what\r\nwent where, nor was grepping necessarily going to tell you as tests for\r\nparticular sections of code were often scattered around multiple test scripts.\r\nAnd, of course, I regularly heard the lament that I've heard at many shops\r\nwith larger codebases: where are the tests for feature *X*? Instead of just\r\nsitting down and writing code, the developers are hunting for the tests,\r\nwondering if there are any tests for the feature they're working on and, if\r\nnot, trying to figure out where to put their new tests.\r\n\r\nUnfortunately, this disorganization is only the start of the problem.\r\n\r\n## Large-scale test suites\r\n\r\nI've worked with many companies with large test suites and they tend to share\r\nsome common problems. I list them in below in the order I try to address these\r\nproblems (in other words, roughly easiest to hardest).\r\n\r\n* Tests often emit warnings\r\n* Tests often fail (\"oh, that sometimes fails. Ignore it.\")\r\n* There is little evidence of organization\r\n* Much of the testing code is duplicated\r\n* Testing fixtures are frequently not used (or poorly used)\r\n* Code coverage is spotty\r\n* They take far too long to run\r\n\r\nProblems are one thing, but what features do we want to see in large-scale\r\ntest suites?\r\n\r\n* Tests should be very easy to write and run\r\n* They should run relatively quickly\r\n* The order in which tests run should not matter\r\n* Test output should be clean\r\n* It should be obvious where to find tests for a particular piece of code\r\n* Testing code should not be duplicated\r\n* Code coverage should be able to analyze different aspects of the system\r\n\r\nLet's take a look at some of the problems and try to understand their impacts.\r\nWhile it's good to push a test suite into a desirable state, often this is\r\nrisky if the underlying problems are ignored. I will offer recommendations for\r\nresolving each problem, but it's important to understand that these are\r\n*recommendations*. They may not apply to your situation.\r\n\r\n### Tests often emit warnings\r\n\r\nThis seems rather innocuous. Sure, code emits warnings and we're used to that.\r\nUnfortunately, we sometimes forget that warnings are *warnings*: there might\r\nvery well be something wrong. In my time at the BBC, one of the first things I\r\ndid was try to clean up all of the warnings. One was a normal warning about\r\nuse of an undefined variable, but it was unclear to me from the code if this\r\nshould be an acceptable condition. Another developer looked at it with me and\r\nrealized that the variable should never be undefined: this warning was masking\r\na very serious bug in the code, but the particular condition was not\r\nexplicitly tested. By rigorously eliminating all warnings, we found it easier\r\nto make our code more correct, and in those places where things were dodgy,\r\ncomments were inserted into the code to explain why warnings were suppressed.\r\nIn short: the code became easier to maintain.\r\n\r\nAnother issue with warnings in the test suite is that they condition\r\ndevelopers to ignore warnings. We get so used to them that we stop reading\r\nthem, even if something serious is going on (on a related note, I often listed\r\nto developers complain about stack traces, but a careful reading of a stack\r\ntrace will often reveal the exact cause of the exception). New warnings crop\r\nup, warnings change, but developers conditioned to ignore them often overlook\r\nserious issues with their code.\r\n\r\n**Recommendation**: Eliminate all warnings from your test suite, but\r\ninvestigate each one to understand if it reflects a serious issue. Also, some\r\ntests will capture STDERR, effectively hiding warnings. Making warnings fatal\r\nwhile running tests can help to overcome this problem.\r\n\r\n### Tests often fail (\"oh, that sometimes fails. Ignore it.\")\r\n\r\nFor one client, their hour-long test suite had many failing tests. When I\r\nfirst started working on it, I had a developer walk me through all of the\r\nfailures and explain why they failed and why they were hard to fix. Obviously\r\nthis is a far more serious problem than warnings, but in the minds of the\r\ndevelopers, they were under constant deadline pressures and as far as\r\nmanagement was concerned, the test suite was practically a luxury, not\r\n\"serious code.\" As a result, developers learned to recognize these failures\r\nand consoled themselves with the thought that they understood the underlying\r\nissues.\r\n\r\nOf course, that's not really how it works. The developer explaining the test\r\nfailures admitted that he didn't understand some of them and with longer test\r\nsuites that routinely fail, more failures tend to crop up, but developers\r\nconditioned to accept failures tend not to notice them. They kick off the test\r\nsuite, run and grab some coffee and later glance over results to see if they\r\nlook reasonable (that's assuming they run all of the tests, something which\r\noften stops happening at this point). What's worse, continuous integration\r\ntools are often built to accomodate this. From the Jenkin's [xUnit Plugin\r\npage](https://wiki.jenkins-ci.org/display/JENKINS/xUnit+Plugin):\r\n\r\n> # Features\r\n>  * Records xUnit tests\r\n>  * Mark the build unstable or fail according to threshold values\r\n\r\nIn other words, there's an \"acceptable\" level of failure. What's the\r\nacceptable level of failure when you debit someone's credit card, or you're\r\nsending their medical records to someone, or you're writing embedded software\r\nthat can't be easily updated?\r\n\r\nDogmatism aside, you can make a case for acceptable levels of test failure,\r\nbut you need to understand the risks and be prepared to accept them. However,\r\nfor the purposes of this document, we'll assume that the acceptable level of\r\nfailure is zero.\r\n\r\nIf you absolutely cannot fix a particular failure, you should at least mark\r\nthe test as `TODO` so that the test suite can pass. Not only does this help to\r\nguide you to a clean test suite, the `TODO` reason is generally embedded in\r\nthe test, giving the next developer a clue what's going on.\r\n\r\n**Recommendation**: Do not allow any failing tests. If tests fail which do not\r\nimpact the correctness of the application (such as documentation or \"coding\r\nstyle\" tests), they should be separated from your regular tests in some manner\r\nand your systems should recognize that it's OK for them to fail.\r\n\r\n### There is little evidence of organization\r\n\r\nAs mentioned previously, a common lament amongst developers is the difficulty\r\nof finding tests for the code they're working on. Consider the case of\r\n[HTML::TokeParser::Simple](http://search.cpan.org/dist/HTML-TokeParser-Simple/).\r\nThe library is organized like this:\r\n\r\n    lib/\r\n    └── HTML\r\n        └── TokeParser\r\n            ├── Simple\r\n            │   ├── Token\r\n            │   │   ├── Comment.pm\r\n            │   │   ├── Declaration.pm\r\n            │   │   ├── ProcessInstruction.pm\r\n            │   │   ├── Tag\r\n            │   │   │   ├── End.pm\r\n            │   │   │   └── Start.pm\r\n            │   │   ├── Tag.pm\r\n            │   │   └── Text.pm\r\n            │   └── Token.pm\r\n            └── Simple.pm\r\n\r\nThere's a class in there named\r\n`HTML::TokeParser::Simple::Token::ProcessInstruction`. Where, in the following\r\ntests, would you find the tests for process instructions?\r\n\r\n    t\r\n    ├── constructor.t\r\n    ├── get_tag.t\r\n    ├── get_token.t\r\n    ├── internals.t\r\n    └── munge_html.t\r\n\r\nYou might think it's in the `get_token.t` test, but are you sure? And\r\nwhat's that strange `munge_html.t` test? Or the `internals.t` test? As\r\nmentioned, for a small library, this really isn't too bad. However, what if we\r\nreorganized our tests to reflect our library hierarchy?\r\n\r\n    t/\r\n    └── tests/\r\n        └── html/\r\n            └── tokeparser/\r\n                ├── simple/\r\n                │   ├── token/\r\n                │   │   ├── comment.t\r\n                │   │   ├── declaration.t\r\n                │   │   ├── tag/\r\n                │   │   │   ├── end.t\r\n                │   │   │   └── start.t\r\n                │   │   ├── tag.t\r\n                │   │   └── text.t\r\n                │   └── token.t\r\n                └── simple.t \r\n\r\nIt's clear that the tests for `HTML::TokeParser::Simple::Token::Tag::Start`\r\nare in `t/tests/html/tokeparser/simple/token/tag/start.t`. And you can see\r\neasily that there is no file for `processinstruction.t`. This test\r\norganization not only makes it easy to find where your tests are, it's also\r\neasy to program your editor to automatically switch between the code and the\r\ntests for the code. For large test suites, this saves a huge amount of time.\r\nWhen I reorganized the test suite of the BBC's central metadata repository,\r\n[PIPs](http://www.bbc.co.uk/blogs/bbcinternet/2009/02/what_is_pips.html), I\r\nfollowed a similar pattern and it made our life much easier.\r\n\r\n(**Note**: the comment about programming your editor is important. Effective\r\nuse of your editor/IDE is one of the most powerful tools in a developer's\r\ntoolbox.)\r\n\r\nOf course, your test suite could easily be more complicated and your top-level\r\ndirectories inside of your test directory may be structured differently:\r\n\r\n    t\r\n    ├── unit/\r\n    ├── integration/\r\n    ├── api/\r\n    └── web/\r\n\r\n**Recommendation**: Organize your test files to have a predictable,\r\ndiscoverable structure. The test suite should be much easier to work with.\r\n\r\n### Much of the testing code is duplicated\r\n\r\nWe're aghast that that people routinely cut-n-paste their application code,\r\nbut we don't even notice when people do this in their test code. More than\r\nonce I've worked on a test suite with a significant logic change and I've had\r\nto find this duplicated code and either change it many places or try to\r\nrefactor it so that it's in a single place and then change it. We already know\r\nwhy duplicated code is bad, I'm unsure why we tolerate this in test suites.\r\n\r\nMuch of my work in tests has been to reduce this duplication. For example,\r\nmany test scripts lists the same set of modules at the top. I did a heuristic\r\nanalysis of tests on the CPAN and chose the most popular testing modules and\r\nthat allowed me to change this:\r\n\r\n    use strict;\r\n    use warnings;\r\n    use Test::Exception;\r\n    use Test::Differences;\r\n    use Test::Deep;\r\n    use Test::Warn;\r\n    use Test::More tests => 42;\r\n\r\nTo this:\r\n\r\n    use Test::Most tests => 42;\r\n\r\nYou can easily use similar strategies to bundle up common testing modules into\r\na single testing module that all of your tests use. Less boilerplate and you\r\ncan easily dive into testing.\r\n\r\nOr as a more egregious example, I often see something like this (a silly\r\nexample just for illustration purposes):\r\n\r\n    set_up_some_data($id);\r\n    my $object = Object->new($id);\r\n\r\n    is $object->attr1, $expected1, 'attr1 works';\r\n    is $object->attr2, $expected2, 'attr2 works';\r\n    is $object->attr3, $expected3, 'attr3 works';\r\n    is $object->attr4, $expected4, 'attr4 works';\r\n    is $object->attr5, $expected5, 'attr5 works';\r\n\r\nAnd then a few lines later:\r\n\r\n    set_up_some_data($new_id);\r\n    $object = Object->new($new_id);\r\n\r\n    is $object->attr1, $new_expected1, 'attr1 works';\r\n    is $object->attr2, $new_expected2, 'attr2 works';\r\n    is $object->attr3, $new_expected3, 'attr3 works';\r\n    is $object->attr4, $new_expected4, 'attr4 works';\r\n    is $object->attr5, $new_expected5, 'attr5 works';\r\n\r\nAnd then a few lines later, the same thing ...\r\n\r\nAnd in another test file, the same thing ...\r\n\r\nPut that in its own test function and wrap those attribute tests in a loop. If\r\nthis pattern is repeated in different test files, put it in a custom test\r\nlibrary:\r\n\r\n    sub test_fetching_by_id {\r\n        my ( $class, $id, $tests ) = @_;\r\n        my $object = $class->new($id);\r\n\r\n        # this causes diagnostics to display the file and line number of the\r\n        # caller on failure, rather than reporting *this* file and line number\r\n        local $Test::Builder::Level = $Test::Builder::Level + 1;\r\n\r\n        foreach my $test (@$tests) {\r\n            my ( $attribute, $expected ) = @$test;\r\n            is $object->$attribute, $expected, \"$attribute works for $class $id\";\r\n        }\r\n    }\r\n\r\nAnd then you call it like this:\r\n\r\n    my @id_tests = (\r\n        { id => $id,\r\n          tests => [\r\n            [ attr1 => $expected1 ],\r\n            [ attr2 => $expected2 ],\r\n            [ attr3 => $expected3 ],\r\n            [ attr4 => $expected4 ],\r\n            [ attr5 => $expected5 ],\r\n        ]},\r\n        { id => $new_id,\r\n          tests  => [\r\n            [ attr1 => $new_expected1 ],\r\n            [ attr2 => $new_expected2 ],\r\n            [ attr3 => $new_expected3 ],\r\n            [ attr4 => $new_expected4 ],\r\n            [ attr5 => $new_expected5 ],\r\n        ]},\r\n    );\r\n\r\n    for my $test ( @id_tests ){\r\n          test_fetching_by_id( 'Object', $test->{id}, $tests->{test} );\r\n    }\r\n\r\nThis is a cleanly refactored data-driven approach. By not repeating yourself,\r\nif you need to test new attributes, you can just add an extra line to the data\r\nstructures and the code remains the same. Or, if you need to change the logic,\r\nyou only have one spot in your code where this is done. Once a developer\r\nunderstands the `test_fetching_by_id()` function, they can reuse this\r\nunderstanding in multiple places. Further, it makes it easier to find patterns\r\nin your code and any competent programmer is always on the lookout for\r\npatterns because those are signposts leading to cleaner designs.\r\n\r\n**Recommendation**: Keep your test code as clean as your application code.\r\n\r\n### Testing fixtures are frequently not used (or poorly used)\r\n\r\nOne difference between your application code and the test suite is in an\r\napplication, we often have no idea what the data will be and we try to have a\r\nclean separation of data and code.\r\n\r\nIn your test suite, we also want a clean separation of data and code (in my\r\nexperience, this is very hit-or-miss), but we often *need* to know the data we\r\nhave. We set up data to run tests against to ensure that we can test various\r\nconditions. Can we give a customer a birthday discount if they were born\r\nFebruary 29th? Can a customer with an overdue library book check out another?\r\nIf our employee number is no longer in the database, is our code properly\r\ndeleted, along with the backups and the git history erased? (kidding!)\r\n\r\nWhen we set up the data for these known conditions under which to test, we\r\ncall the data a [test fixture](http://en.wikipedia.org/wiki/Test_fixture).\r\nTest fixtures, when properly designed, allow us generate clean,\r\nunderstandable tests and make it easy to write tests for unusual conditions\r\nthat may otherwise be hard to analyze.\r\n\r\nThere are several common anti-patterns I see in fixtures.\r\n\r\n* Hard to set up and use\r\n* Adding them to the database and not rolling them back\r\n* Loading all your test data at once with no granularity\r\n\r\nIn reviewing various fixture modules on the CPAN and for clients I have worked\r\nwith, much of the above routinely holds true. On top of that, documentation is\r\noften rather sparse or non-existent. Here's a (pseudo-code) example of an\r\nalmost undocumented fixture system for one client I worked with and it\r\nexemplified common issues in this area.\r\n\r\n    load_fixture(\r\n        database => 'sales',\r\n        client   => $client_id,\r\n        datasets => [qw/ customers orders items order_items referrals /],\r\n    );\r\n\r\nThis had several problems, all of which could be easily corrected *as code*,\r\nbut they built a test suite around these problems and had backed themselves\r\ninto a corner, making their test suite dependent on bad behavior.\r\n\r\nThe business case is that my client had a product serving multiple customers\r\nand each customer would have multiple separate databases. In the above,\r\nclient *$client_id* connects to their sales database and we load several test\r\ndatasets and run tests against them. However, loading of data was not done in\r\na transaction, meaning that there was no isolation between different test\r\ncases in the same process. More than once I caught issues where running an\r\nindividual test case would often fail because it depended on data loaded by a\r\ndifferent test case, but it wasn't always clear which test cases were coupled\r\nwith which.\r\n\r\nAnother issue is that fixtures were not fine-tuned to address particular test\r\ncases. Instead, if you loaded \"customers\" or \"referrals\", you got *all* of\r\nthem in the database. Do you need a database with a single customer with a\r\nsingle order and only one order item on it to test that obscure bug that\r\noccurs when a client first uses your software? There really wasn't any clean\r\nway of doing that; data was loaded in an \"all or nothing\" context. Even if you\r\nviolated the paradigm and tried to create fine-tuned fixtures, it was very,\r\nvery hard to write them due to the obscure, undocumented format needed to\r\ncraft the data files for them.\r\n\r\nBecause transactions were not used and changes could not be rolled back, each\r\n`*.t` file would rebuild its own test database, a very slow process. Further,\r\ndue to lack of documentation about the fixtures, it was often difficult to\r\nfigure out which combination of fixtures to load to test a given feature. Part\r\nof this is simply due to the complex nature of the business rules, but the\r\ncore issues stemmed from a poor understanding of fixtures. This client now has\r\nmultiple large, slow test suites, spread across multiple repositories, all of\r\nwhich constantly tear down and set up databases and load large amounts of\r\ndata. The test suites are both slow and fragile The time and expense to fix\r\nthis problem is considerable due to how long they've pushed forward with this\r\nsubstandard setup.\r\n\r\nWhat you generally want is the ability to easily create understandable\r\nfixtures which are loaded in a transaction, tests are run, and then changes\r\nare rolled back.  The fixtures need to be fine-grained so you can tune them\r\nfor a particular test case.\r\n\r\nOne attempt I've made to fix this situation is releasing\r\n[DBIx::Class::EasyFixture](http://search.cpan.org/dist/DBIx-Class-EasyFixture/lib/DBIx/Class/EasyFixture.pm),\r\nalong with [a tutorial](http://search.cpan.org/dist/DBIx-Class-EasyFixture/lib/DBIx/Class/EasyFixture/Tutorial.pm).\r\nIt does rely on `DBIx::Class`, the most popular ORM for Perl. This will likely\r\nmake it unsuitable for some use cases.\r\n\r\nUsing them is very simple:\r\n\r\n    my $fixtures = DBIx::Class::EasyFixture->new(schema => $schema);\r\n    $fixtures->load('customer_with_order_without_items');\r\n\r\n    # run your tests\r\n\r\n    $fixtures->unload; # also unloads when out of scope\r\n\r\nFor the customer's code, we could satisfy the different database requirements\r\nby passing in different schemas. Other (well-documented) solutions,\r\nparticularly those which are pure `DBI` based are welcome in this area.\r\n\r\n**Recommendation**: Fine-grained, well-documented fixtures which are easy to\r\ncreate and easy to clean up.\r\n\r\n### Code coverage is poorly understood\r\n\r\nConsider the following code:\r\n\r\n    float recip(float number) {\r\n        return 1.0 / number;\r\n    }\r\n\r\nAnd a sample test:\r\n\r\n    assert recip(2.0) returns .5;\r\n\r\nCongratulations! You now have 100% code coverage of that function.\r\n\r\nFor a statically typed language, I'm probably going to be moderately\r\ncomfortable with that test. Alas, for dynamically typed languages we're\r\nfooling ourselves. An equivalent function in Perl will pass that test if we\r\nuse `recip(\"2 apples\")` as the argument. And what happens if we pass a file\r\nhandle? And would a Unicode number work? What happens if we pass no arguments?\r\nPerl is powerful and lets us write code quickly, but there's a price: it\r\nexpects us to know what we're doing and passing unexpected kinds of data is a\r\nvery common source of errors, but one that 100% code coverage will never (no\r\npun intended) uncover. This can lead to false confidence.\r\n\r\nTo work around false confidence in your code, always assume that you write\r\napplications to create things and you write tests to destroy them. Testing is,\r\nand should be, an act of violence. If you're not breaking anything with your\r\ntests, you're probably doing it wrong.\r\n\r\nOr what if you have that code in a huge test suite, but it's dead code? We\r\ntend to blindly run code coverage over our entire test suite, never\r\nconsidering whether or not we're testing dead code. This is because we slop\r\nour unit, integration, API and other tests all together.\r\n\r\nOr consider the following test case:\r\n\r\n    sub forum : Tests(1) {\r\n        my $self = shift;\r\n        my $site = $self->test_website;\r\n        $site->login($user, $pass);\r\n        $site->get('/forum');\r\n        $site->follow_link( text => 'Off Topic' );\r\n        $site->post_ok({\r\n            title => 'What is this?',\r\n            body  => 'This is a test'.\r\n        }, 'We should be able to post to the forum');\r\n    }\r\n\r\n`Devel::Cover` doesn't know which code is test code and which is not.\r\n`Devel::Cover` merely tells you if your application code was exercised in your\r\ntests. [You can annotate your code with \"uncoverable\"\r\ndirectives](http://search.cpan.org/dist/Devel-Cover/lib/Devel/Cover.pm#UNCOVERABLE_CRITERIA)\r\nto tell `Devel::Cover` to ignore the following code, but that potentially\r\nmeans sprinkling your code with annotations all over the place.\r\n\r\nThere are multiple strategies to deal with this. One of the simplest is to\r\nmerely run your code coverage tools over the public-facing portions of your\r\ncode, such as web or API tests. If you find uncovered code, you either have\r\ncode that is not fully tested (in the sense that you don't know if your API\r\ncan really use that code) or, if you cannot write an API test to reach that\r\ncode, investigate if it is dead code.\r\n\r\nYou can do this by grouping your tests into subdirectories:\r\n\r\n    t/\r\n    |--api/\r\n    |--integration/\r\n    `--unit/\r\n\r\nAlternatively, if you use `Test::Class::Moose`, you can tag your tests and\r\nonly run coverage over tests including the tags you wish to test:\r\n\r\n    My::Test::Class::Moose->new({\r\n      include_tags => [qw/api/],\r\n    })->runtests;\r\n\r\nIf you start tagging your tests by the subsystems they are testing, you can\r\nthen start running code coverage on specific subsystems to determine which\r\nones are poorly tested.\r\n\r\n**Recommendation**: Run coverage over public-facing code and on different\r\nsubsystems to find poor coverage.\r\n\r\n### They take far too long to run\r\n\r\nThe problem with long-running test suites is well known, but it's worth\r\ncovering this again here. These are problems that others have discussed and\r\nthat I have also personally experienced many times.\r\n\r\n![Perl's version of waiting for a compile](http://i.imgur.com/JNfyxoo.png)\r\n\r\n*With apologies to [XKCD](http://xkcd.com/303/)*\r\n\r\nIn the best case scenario for developers who always run that long-running test\r\nsuite, expensive developer time is wasted while the test suite is running.\r\nWhen they launch that hour-long (or more) test suite, they frequently take a\r\nbreak, talk to (read: interrupt) other developers, check their Facebook, or do\r\nany number of things which equate to \"not writing software.\" Yes, some of\r\nthose things involve meetings or research, but meetings don't conveniently\r\nschedule themselves when we run tests and for mature products (those which are\r\nmore likely to have long-running test suites), there's often not that much\r\nresearch we really need to do.\r\n\r\nHere are some of the issues with long-running test suites:\r\n\r\n* Expensive developer time is wasted while the test suite runs\r\n* Developers often don't run the entire test suite\r\n* Expensive code coverage is not generated as a result\r\n* Code is fragile as a result\r\n\r\nWhat I find particularly curious is that we accept this state of affairs. Even\r\na back-of-the-envelope calculation can quickly show significant productivity\r\nbenefits that will pay off in the long run by taking care of our test suite.\r\n[I once reduced a BBC test suite's run time from one hour and twenty minutes down\r\nto twelve\r\nminutes](http://www.slideshare.net/Ovid/turbo-charged-test-suites-presentation)\r\n(*Note: today I use a saner approach that results in similar or greater\r\nperformance benefits*).  We had six developers on that team. When the test\r\nsuite took over an hour to run, they often didn't run the test suite. They\r\nwould run tests on their section of code and push their code when they were\r\ncomfortable with it. This led to other developers finding buggy code and\r\nwasting time trying to figure out how they broken it when, in fact, someone\r\nelse broke the code.\r\n\r\nBut let's assume each developer was running the test suite at least once a day\r\n(I'm careful about testing and often ran mine twice a day). By cutting test\r\nsuite run time by over an hour, we reclaimed a *full day* of developer\r\nproductivity every day! Even if it takes a developer a month to increase\r\nperfomance by that amount it pays for itself many times over very quickly.\r\nWhy would you not do this?  As a business owner, wouldn't you want your\r\ndevelopers to save time on their test suite so they can create features faster\r\nfor you?\r\n\r\nThere are several reasons why this is difficult. Tasking a developer with a\r\nblock of time to speed up a test suite means the developer is not creating\r\nuser-visible features during that time. For larger test suites, it's often\r\nimpossible to know in advance just how much time you can save or how long it\r\nwill take you to reach your goal. In most companies I've worked with, the\r\npeople who can make the decision to speed up the test suite are often not the\r\npeople feeling the pain. Productivity and quality decrease slowly over time,\r\nleading to the [boiling frog\r\nproblem](http://en.wikipedia.org/wiki/Boiling_frog).\r\n\r\nWhat's worse: in order to speed up your test suite without affecting behavior,\r\nthe test suite often has to be \"fixed\" (eliminating warnings, failures, and\r\nreducing duplication) to ensure that no behavior has been changed during the\r\nrefactor.\r\n\r\nFinally, some developers simply don't have the background necessary to\r\nimplement performance optimizations. While performance profiles such as Perl's\r\n[Devel::NYTProf](http://search.cpan.org/dist/Devel-NYTProf/lib/Devel/NYTProf.pm)\r\ncan easily point out problem areas in the code, it's not always clear how to\r\novercome the discovered limitations.\r\n\r\nThe single biggest factor in poor test suite performance for applications is\r\nfrequently I/O. In particular, working with the database tends to be a\r\nbottleneck and there's only so much database tuning that can be done. After\r\nyou've profiled your SQL and optimized it, several database-related\r\noptimizations which can be considered are:\r\n\r\n1. Using transactions to clean up your database rather than rebuilding the\r\n   database\r\n2. Only connect to the database once per test suite (hard when you're using\r\n   a separate processe per test file)\r\n3. If you must rebuild the database, maintain a pool of test databases and\r\n   assign them as needed, rebuilding used ones in the background\r\n4. Use smaller database fixtures instead of loading everything at once\r\n\r\nAfter you've done all you can to improve your database access, you may find\r\nthat your test suite is \"fast enough\", but if you wish to go further, there\r\nare several steps you can take.\r\n\r\n#### Use [Test::Aggregate](http://search.cpan.org/dist/Test-Aggregate/)\r\n\r\n`Test::Aggregate` can often double the speed of your test suite (I've had it\r\nspeed up test suites by around 65%). It does this by taking your separate\r\n`*.t` files and runs them in a single process. Not all tests can be run this\r\nway (tests that munge global state without cleaning up are prime examples),\r\nbut it's the easiest way to get a quick boost to test suite performance.\r\n\r\n#### Aggressively search for and remove duplicated tests.\r\n\r\nFor poorly organized test suites, developers sometimes make the mistake of\r\nputting tests for something in a new `*.t` file or add them to a different\r\n`*.t` file, even if related tests already exist. This strategy can be\r\ntime-consuming and often does not result in quick wins.\r\n\r\n#### Use `Devel::NYTProf` aggressively.\r\n\r\nFor one test suite, I found that we were using a pure Perl implementation of\r\nJSON. As the test suite used JSON extensively, switching to\r\n[JSON::XS](http://search.cpan.org/dist/JSON-XS/XS.pm) gave us a nice\r\nperformance boost. We may not have noticed that if we hadn't been profiling\r\nour code.\r\n\r\n#### Look for code with \"global\" effects\r\n\r\nOn one test suite, I ensured that `Universal::isa` and `Universal::can` cannot\r\nbe loaded. It was a quick fix and sped up the test suite by 2% (several small\r\naccumulations of improvements can add up quickly).\r\n\r\n#### Inline \"hot\" functions.\r\n\r\nConsider the following code which runs in about 3.2 seconds on my computer:\r\n\r\n    #!/usr/bin/env perl\r\n\r\n    use strict;\r\n    use warnings;\r\n    no warnings 'recursion';\r\n\r\n    for my $i ( 1 .. 40 ) {\r\n        for my $j ( 1 .. $i**2 ) {\r\n            my $y = factorial($j);\r\n        }\r\n    }\r\n\r\n    sub factorial {\r\n        my $num = shift;\r\n        return 1 if $num <= 1;\r\n        return $num * factorial($num - 1);\r\n    }\r\n\r\nBy rewriting the recursive function as a loop, the code takes about .87\r\nseconds:\r\n\r\n    sub factorial {\r\n        my $num = shift;\r\n        return 1 if $num <= 1;\r\n        $num *= $_ for 2 .. $num - 1;\r\n        return $num;\r\n    }\r\n\r\nBy inlining the calculation, the code completes in .69 seconds:\r\n\r\n    for my $i ( 1 .. 40 ) {\r\n        for my $j ( 1 .. $i**2 ) {\r\n            my $y = $j;\r\n            if ( $y > 1 ) {\r\n                $y *= $_ for 2 .. $y - 1;\r\n            }\r\n        }\r\n    }\r\n\r\nIn other words, in our trivial example, the inlined behavior is roughly 20%\r\nfaster than the iterative function and 80% faster than the recursive function.\r\n\r\n#### Recompile your Perl\r\n\r\nYou may wish to recompile your Perl to gain a performance improvement. Many\r\nLinux distributions ship with a threaded Perl by default. Depending on the\r\nversion of Perl you ship with, you can gain performance improvements of up to\r\n30% by recompiling without threads. Of course, if you use threads, you'll feel\r\nvery stupid for doing this. However, if you don't make heavy use of threads,\r\nswitching to a forking model for the threaded code may make the recompile\r\nworth it. Naturally, you'll need to heavily benchmark your code (preferably\r\nunder production-like loads) to understand the trade-offs here.\r\n\r\n#### Preload modules\r\n\r\nIf your codebase makes heavy use of modules that are slow to load, such as\r\n`Moose`, `Catalyst`, `DBIx::Class` and others, preloading them might help.\r\n[forkprove](http://search.cpan.org/~miyagawa/forkprove-v0.4.9/script/forkprove)\r\nis a utility written by Tatsuhiko Miyagawa that allows you to preload\r\nslow-loading modules and then forks off multiple processes to run your tests.\r\nUsing this tool, [I reduced one sample test suite's run time from 12 minutes\r\nto about a\r\nminute](http://blogs.perl.org/users/ovid/2013/12/merry-christmas-parallel-testing-with-testclassmoose-has-arrived.html).\r\nUnfortunately, `forkprove` doesn't allow schedules, a key component often\r\nneeded for larger test suites. I'll explain that in the next section.\r\n\r\n#### Parallel tests\r\n\r\nRunning tests in parallel is tricky. Some tests simply *can't* be run with\r\nother tests. Usually these are tests which alter global state in some manner\r\nthat other processes will pick up, or might cause resource starvation of some\r\nkind.\r\n\r\nOr some tests *can* be run in parallel with other tests, but if several tests\r\nare updating the same records in the database at the same time, locking\r\nbehavior might slow down the tests considerably.\r\n\r\nOr maybe you're running 4 jobs, but all of your slowest tests are grouped in\r\nthe same job: not good.\r\n\r\nTo deal with this, you can create a schedule that assigns different tests to\r\ndifferent jobs, based on a set of criteria, and then puts tests which cannot\r\nrun in parallel in a single job that runs after the others have completed.\r\n\r\nYou can use\r\n[TAP::Parser::Scheduler](http://search.cpan.org/dist/Test-Harness/lib/TAP/Parser/Scheduler.pm)\r\nto create an effective parallel testing setup. You can use this with\r\n`TAP::Parser::Multiplexer` to create your parallel tests. Unfortunately, as of\r\nthis writing there's a bug in the Multiplexer whereby it uses `select` in a\r\nloop to read the parser output. If one parser blocks, none of the other output\r\nis read. Further, the schedule must be created prior to loading your test\r\ncode, meaning that if your tests would prefer a different schedule, you're out\r\nof luck. Also, `make test` currently doesn't handle this well. There is work\r\nbeing done by David Golden to alleviate this problem.\r\n\r\nMy preferred solution is to use\r\n[Test::Class::Moose](http://search.cpan.org/dist/Test-Class-Moose/). That has\r\nbuilt-in parallel testing and writing schedules is very easy. Further,\r\ndifferent test cases can simply use a `Tags(noparallel)` attribute to ensure\r\nthat they're run sequentially after the parallel tests.\r\n\r\nAside from the regular benefits of `Test::Class::Moose`, an interesting\r\nbenefit of this module is that it loads all of your test and application code\r\ninto a single process and *then* forks off subprocesses. As a result, your\r\ncode is loaded once and only once. Alternate strategies which try to fork\r\nbefore loading your code might still cause the code to be loaded multiple\r\ntimes.\r\n\r\nI have used this strategy to reduce a [12 minute test suite to 30\r\nseconds](http://blogs.perl.org/users/ovid/2013/12/merry-christmas-parallel-testing-with-testclassmoose-has-arrived.html).\r\n\r\n#### Distributed tests\r\n\r\nThough I haven't used this module, Alex Vandiver has written\r\n[TAP::Harness::Remote](http://search.cpan.org/dist/TAP-Harness-Remote/lib/TAP/Harness/Remote.pm).\r\nThis module allows you to rsync directory trees to multiple servers and run\r\ntests on those servers. Obviously, this requires multiple servers.\r\n\r\nIf you want to roll your own version of this, I've also released\r\n[TAP::Stream](http://search.cpan.org/dist/TAP-Stream/), a module that allows\r\nyou to take streams (the text, actually) of TAP from multiple sources and\r\ncombine them into a single TAP document.\r\n\r\n#### Devel::CoverX::Covered\r\n\r\nThere is yet another interesting strategy: only run tests that exercise the\r\ncode that you're changing. Johan Lindström wrote\r\n[Devel::CoverX::Covered](http://search.cpan.org/dist/Devel-CoverX-Covered/).\r\nThis modules is used in conjunction with Paul Johnson's\r\n[Devel::Cover](http://search.cpan.org/dist/Devel-Cover/) to identify all the\r\nplaces in your tests which cover a particular piece of code. In the past, I've\r\nwritten tools for vim to read this data and only run relevant tests. This is a\r\ngenerally useful approach, but there are a couple of pitfalls.\r\n\r\nFirst, if you test suite takes a long time to run, it will take much, much\r\nlonger to run with `Devel::Cover`. As a result, I recommend that this be used\r\nwith a special nightly \"cover build\" and have the results synched back to the\r\ndevelopers.\r\n\r\nSecond, when changing code, it's easy to change which tests cover your code,\r\nleading to times when this technique won't cover your actual changes\r\nthoroughly. In practice, this hasn't been a problem for me, but I've not used\r\nit enough to say that with confidence.\r\n\r\n**Recommendation**: Don't settle for slow test suites. Pick a goal and work to\r\nachieving that goal (it's easy to keep optimizing for too long and start\r\ngetting diminishing marginal returns).\r\n\r\n# [Test::Class::Moose](https://github.com/Ovid/test-class-moose)\r\n\r\nIf you start creating a large Web site, do you start writing a bunch of\r\nindividual scripts, each designed to handle one URL and each handling their\r\nown database access and printing their output directly to STDOUT? Of course\r\nnot. Today, professional developers reach for Sinatra, Seaside, Catalyst, Ruby\r\non Rails or other Web frameworks. They take a bit more time to set up and\r\nconfigure, but we know they generally save more time in the long run. Why\r\nwouldn't you do that with your test suite?\r\n\r\nIf you're using Perl, many of the problems listed in this document can be\r\navoided by switching to `Test::Class::Moose`. This is a testing framework I\r\ndesigned to make it very easy to test applications. Once you understand it,\r\nit's actually easy to use for testing libraries, but it really shines for\r\napplication testing.\r\n\r\nNote that I now regret putting `Moose` in the name. `Test::Class::Moose` is a\r\nrewrite of `Test::Class` using `Moose`, but it's *not* limited to testing\r\n`Moose` applications. It uses `Moose` because internally it relies on the\r\n`Moose` meta-object protocol for introspection.\r\n\r\nOut of the box you get:\r\n\r\n* Reporting\r\n* Parallel tests (which optionally accepts a custom schedule)\r\n* Tagging tests (slice and dice your test suite!)\r\n* Test inheritance (xUnit for the win!)\r\n* Full Moose support\r\n* Test control methods (startup, setup, teardown, shutdown)\r\n* Extensibility\r\n* All the testing functions and behavior from `Test::Most`\r\n\r\nTo learn about xUnit testing in Perl, you may wish to read a five-part\r\ntutorial I published at Modern Perl Books:\r\n\r\n1. [Organizing test suites with Test::Class](http://www.modernperlbooks.com/mt/2009/03/organizing-test-suites-with-testclass.html)\r\n2. [Reusing test code](http://www.modernperlbooks.com/mt/2009/03/reusing-test-code-with-testclass.html)\r\n3. [Making your testing life easier](http://www.modernperlbooks.com/mt/2009/03/making-your-testing-life-easier.html)\r\n4. [Using test control methods](http://www.modernperlbooks.com/mt/2009/03/using-test-control-methods-with-testclass.html)\r\n5. [Working with Test::Class test suites](http://www.modernperlbooks.com/mt/2009/03/working-with-testclass-test-suites.html)\r\n\r\nThat tutorial is slightly out of date (I wrote it five years ago), but it\r\nexplains effective use of `Test::Class` and some common anti-patterns when\r\nusing it.\r\n\r\nDoug Bell has started [a tutorial for\r\nTest::Class::Moose](http://search.cpan.org/dist/Test-Class-Moose/lib/Test/Class/Moose/Tutorial.pm).\r\nThat also needs updating, but between those and reading the\r\n`Test::Class::Moose` documentation, you should be able to get up to speed\r\nfairly quickly.\r\n\r\n# About The Author\r\n\r\nFor those of you who may be reading this and are not familiar with me, I am\r\nCurtis \"Ovid\" Poe. I authored the test harness that ships with the Perl\r\nprogramming language. I wrote the well-reviewed book [Beginning\r\nPerl](http://www.amazon.com/Beginning-Perl-Curtis-Poe/dp/1118013840/ref=sr_1_1?s=books&ie=UTF8&qid=1395074590&sr=1-1&keywords=beginning+perl+curtis+poe&tag=overse-20)\r\nand and am one of the authors of [Perl\r\nHacks](http://www.amazon.com/Perl-Hacks-Programming-Debugging-Surviving/dp/0596526741/)\r\n(how's that for a redundant title?). I also sit on [the Board of Directors of\r\nthe Perl Foundation](http://www.perlfoundation.org/who_s_who) and am one of\r\nthe people behind [All Around The World](http://www.allaroundtheworld.fr/), a\r\ncompany offering software consulting, training, and international IT\r\nrecruiting.\r\n\r\nIf you'd like to hire me to fix your test suite or write software for you,\r\ndrop me a line at\r\n[jobs@allaroundtheworld.fr](mailto:jobs@allaroundtheworld.fr).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}